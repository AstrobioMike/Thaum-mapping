############################################################################################
## Snakefile for mapping metagenomic samples                                              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os


########################################
############# General Info #############
########################################

"""
There are only a few programs, so not using separate conda environments for individual rules.
All meant to be run in a conda environment that can be created as:
# conda create -n Thaum-mapping -c conda-forge -c bioconda -c defaults -c astrobiomike \
#              bowtie2=2.4.2 samtools=1.11 sra-tools=2.10.8 kofamscan=1.3.0 prodigal=2.6.3 \
#              bit=1.8.15 bbmap=38.86 snakemake=5.26.1
# conda activate Thaum-mapping

As written below, the KOFamScan db needs to be stored a shell environmental variable called: $KO_DIR
This can be added to a conda environment like so (when in the active conda environment):
mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d/
echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh

Where the HMMs would be: /path/to/kofamscan_db/profiles/
and the ko_list would be: /path/to/kofamscan_db/ko_list
"""


########################################
######## Setting some variables ########
########################################

  # single-column file holding reference genome IDs (extension expected to be .fa, but not in this file)
ref_genome_IDs = "all-ref-genome-IDs.txt"


  # single-column file holding unique portion of illumina sample names that process one way
sample_IDs_file = "unique-illumina-sample-IDs.txt"
pyro_sample_IDs_file = "unique-454-sample-IDs.txt"

  # single-column file of the Hong Kong MGs because they need to be processed differently (PRJNA588686)
HK_sample_IDs_file = "unique-HK-sample-IDs.txt"

  # tab-delimited mapping file holding our unique sample IDs in first column, 
  # and their corresponding ERR run accessions in the second column (for those being processed first way)
    # if multiple run accessions, they need to be comma-delimited, e.g.: ANE_004_05M	ERR598955,ERR599003
sample_to_mapping_file = "sample-and-run-accessions.tsv"

  # tab-delimited mapping file holding unique HK dataset IDs in first column,
  # and their corresponding SRR run accession for their forward read in the 2nd column,
  # and reverse read in 3rd column, e.g.: AT04-0m-8Ar	SRR10912837	SRR10912836
    # all of these had single accessions for forward and single for reverse
HK_sample_to_mapping_file = "HK-sample-and-run-accessions.tsv"


  # tab-delimited mapping file holding SRR run accession in first column,
  # and corresponding ftp download link in second column (for those that need to be procesed differently),
  #  e.g.: SRR10912786	ftp.sra.ebi.ac.uk/vol1/fastq/SRR109/086/SRR10912786/SRR10912786.fastq.gz
HK_SRR_to_link_mapping_file = "HK-SRRs-and-links.tsv"


  # fasta file of all combined ref genome fasta files
combined_fasta = "all-combined.fa"


  # bowtie2 index base name
bowtie2_basename = "all-combined"
  # bowtie2 index extensions
bowtie2_index_exts = [".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2"]


  # directories
ref_genomes_dir = "../ref-genomes/"
mapping_dir = "../mapping-files/"
HK_mapping_dir = "../mapping-files-HK/"
pyro_mapping_dir = "../mapping-files-pyro/"
genes_and_annotations_dir = "../genes-and-annotations/"
combined_results = "../combined-results/"

dirs_to_create = [mapping_dir, HK_mapping_dir, pyro_mapping_dir, genes_and_annotations_dir, combined_results]


  # number of threads or cpus use PER snakemake job started (which is determined by the -j parameter passed to the snakemake call)
    # passed to bowtie2
num_threads = 4
    # passed to kofamscan
num_cpus = 4

########################################
#### Reading samples file into list ####
########################################

ref_genome_ID_list = [line.strip() for line in open(ref_genome_IDs)]
sample_ID_list = [line.strip() for line in open(sample_IDs_file)]
HK_sample_ID_list = [line.strip() for line in open(HK_sample_IDs_file)]
pyro_sample_ID_list = [line.strip() for line in open(pyro_sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in dirs_to_create:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        bowtie2_db = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts),
        reg_bam_files = expand(mapping_dir + "{ID}.bam", ID = sample_ID_list),
        pyro_bam_files = expand(pyro_mapping_dir + "{ID}.bam", ID = pyro_sample_ID_list),
        HK_bam_files = expand(HK_mapping_dir + "{ID}.bam", ID = HK_sample_ID_list),
        combined_genome_covs = combined_results + "Combined-genome-coverages.tsv",
        combined_genome_dets = combined_results + "Combined-genome-detections.tsv",
        combined_genome_filt_covs = combined_results + "Combined-genome-detection-filtered-coverages.tsv",
        norm_combined_genome_covs = combined_results + "Combined-CPM-normalized-genome-coverages.tsv",
        norm_combined_genome_filt_covs = combined_results + "Combined-CPM-normalized-genome-detection-filtered-coverages.tsv",
        gene_covs = expand(mapping_dir + "{ID}-gene-coverages.tsv", ID = sample_ID_list),
        genome_cov = expand(mapping_dir + "{ID}-genome-level-coverages.tsv", ID = sample_ID_list),
        genome_det = expand(mapping_dir + "{ID}-genome-level-detections.tsv", ID = sample_ID_list),
        filtered_genome_cov = expand(mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = sample_ID_list),
        pyro_gene_covs = expand(pyro_mapping_dir + "{ID}-gene-coverages.tsv", ID = pyro_sample_ID_list),
        pyro_genome_cov = expand(pyro_mapping_dir + "{ID}-genome-level-coverages.tsv", ID = pyro_sample_ID_list),
        pyro_genome_det = expand(pyro_mapping_dir + "{ID}-genome-level-detections.tsv", ID = pyro_sample_ID_list),
        pyro_filtered_genome_cov = expand(pyro_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = pyro_sample_ID_list),
        HK_gene_covs = expand(HK_mapping_dir + "{ID}-gene-coverages.tsv", ID = HK_sample_ID_list),
        HK_genome_cov = expand(HK_mapping_dir + "{ID}-genome-level-coverages.tsv", ID = HK_sample_ID_list),
        HK_genome_det = expand(HK_mapping_dir + "{ID}-genome-level-detections.tsv", ID = HK_sample_ID_list),
        HK_filtered_genome_cov = expand(HK_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = HK_sample_ID_list),
        annotations = genes_and_annotations_dir + "KO-annotations.tsv",
        combined_gene_covs = combined_results + "Combined-gene-coverages-and-KO-annots.tsv",
        normalized_combined_gene_covs = combined_results + "Combined-CPM-normalized-gene-coverages-and-KO-annots.tsv",
        MG_mapping_summary_info_tab = combined_results + "Per-sample-mapping-overview.tsv"


rule make_MG_summary_table:
    """ makes summary table per MG sample pulling info from bowtie2 mapping info files """

    input:
        expand(mapping_dir + "{ID}-mapping-info.txt", ID = sample_ID_list),
        expand(pyro_mapping_dir + "{ID}-mapping-info.txt", ID = pyro_sample_ID_list),
        expand(HK_mapping_dir + "{ID}-mapping-info.txt", ID = HK_sample_ID_list)
    output:
        MG_mapping_summary_info_tab = combined_results + "Per-sample-mapping-overview.tsv"
    shell:
        """
        cat <( printf "Sample\n" ) <( grep "overall" {input} | cut -f 1 -d ":" | cut -f 3 -d "/" | sed 's/-mapping-info.txt//' ) > sample.tmp

        cat <( printf "num_fragments\n") <( grep -m 1 -w "reads" {input} | cut -f 2 -d ":" | cut -f 1 -d " " ) > num_fragments.tmp

        # if the file has the word "paired" in it, they were paired-end, if "unpaired" is in there, it's a single-end dataset
        grep -m 1 -w "paired" {input} | cut -f 1 -d ":" | cut -f 3 -d "/" | sed 's/-mapping-info.txt//' > MGs-with-PE.tmp
        for sample in $(cat MGs-with-PE.tmp); do printf "PE\n"; done > PE-label.tmp
        paste MGs-with-PE.tmp PE-label.tmp > MGs-with-PE-labels.tmp

        grep -m 1 -w "unpaired" {input} | cut -f 1 -d ":" | cut -f 3 -d "/" | sed 's/-mapping-info.txt//' > MGs-with-SE.tmp
        for sample in $(cat MGs-with-SE.tmp); do printf "SE\n"; done > SE-label.tmp
        paste MGs-with-SE.tmp SE-label.tmp > MGs-with-SE-labels.tmp

        cat MGs-with-PE-labels.tmp MGs-with-SE-labels.tmp > MGs-with-PE-or-SE-labels.tmp

        printf "PE/SE\n" > PE-SE.tmp
        for sample in $(tail -n +2 sample.tmp); do grep -m 1 -w ${{sample}} MGs-with-PE-or-SE-labels.tmp | cut -f 2 >> PE-SE.tmp; done
        
        cat <( printf "perc_reads_aligned\n") <( grep "overall" {input} | cut -f 2 -d ":" | cut -f 1 -d "%" ) > perc_reads_aligned.tmp


        paste sample.tmp num_fragments.tmp PE-SE.tmp perc_reads_aligned.tmp > {output}
        rm sample.tmp num_fragments.tmp MGs-with-PE.tmp PE-label.tmp MGs-with-PE-labels.tmp MGs-with-SE.tmp SE-label.tmp MGs-with-SE-labels.tmp MGs-with-PE-or-SE-labels.tmp PE-SE.tmp perc_reads_aligned.tmp
        """


rule combine_genome_covs_and_detections:
    """ generates tables of combined genome-level coverages and detections """

    input:
        reg_illumina_set_covs = expand(mapping_dir + "{ID}-genome-level-coverages.tsv", ID = sample_ID_list),
        pyro_set_covs = expand(pyro_mapping_dir + "{ID}-genome-level-coverages.tsv", ID = pyro_sample_ID_list),
        HK_set_covs = expand(HK_mapping_dir + "{ID}-genome-level-coverages.tsv", ID = HK_sample_ID_list),
        reg_illumina_set_dets = expand(mapping_dir + "{ID}-genome-level-detections.tsv", ID = sample_ID_list),
        pyro_set_dets = expand(pyro_mapping_dir + "{ID}-genome-level-detections.tsv", ID = pyro_sample_ID_list),
        HK_set_dets = expand(HK_mapping_dir + "{ID}-genome-level-detections.tsv", ID = HK_sample_ID_list),
        reg_illumina_set_filt_covs = expand(mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = sample_ID_list),
        pyro_set_filt_covs = expand(pyro_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = pyro_sample_ID_list),
        HK_set_filt_covs = expand(HK_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv", ID = HK_sample_ID_list)
    output:
        combined_genome_covs = combined_results + "Combined-genome-coverages.tsv",
        combined_genome_dets = combined_results + "Combined-genome-detections.tsv",
        combined_genome_filt_covs = combined_results + "Combined-genome-detection-filtered-coverages.tsv",
        norm_combined_genome_covs = combined_results + "Combined-CPM-normalized-genome-coverages.tsv",
        norm_combined_genome_filt_covs = combined_results + "Combined-CPM-normalized-genome-detection-filtered-coverages.tsv"
    shell:
        """
        python scripts/combine-genome-level-cov-and-detections.py --input-cov-tables {input.reg_illumina_set_covs} {input.pyro_set_covs} {input.HK_set_covs} \
                                                                  --input-det-tables {input.reg_illumina_set_dets} {input.pyro_set_dets} {input.HK_set_dets} \
                                                                  --input-filtered-cov-tables {input.reg_illumina_set_filt_covs} {input.pyro_set_filt_covs} {input.HK_set_filt_covs} \
                                                                  -o {combined_results}Combined
        """


rule combine_gene_covs_and_annots:
    """ generates tables of combined gene coverages with annotations """

    input:
        reg_illumina_set = expand(mapping_dir + "{ID}-gene-coverages.tsv", ID = sample_ID_list),
        pyro_set = expand(pyro_mapping_dir + "{ID}-gene-coverages.tsv", ID = pyro_sample_ID_list),
        HK_set = expand(HK_mapping_dir + "{ID}-gene-coverages.tsv", ID = HK_sample_ID_list),
        annotations = genes_and_annotations_dir + "KO-annotations.tsv"
    output:
        combined_gene_covs = combined_results + "Combined-gene-coverages-and-KO-annots.tsv",
        normalized_combined_gene_covs = combined_results + "Combined-CPM-normalized-gene-coverages-and-KO-annots.tsv"
    shell:
        """
        python scripts/combine-gene-level-coverages.py {input.reg_illumina_set} {input.pyro_set} {input.HK_set} -a {input.annotations} -o {combined_results}Combined
        """


rule call_genes:
    """ calls genes on ref genomes with prodigal """

    input:
        ref_genomes_dir + combined_fasta
    output:
        AA = genes_and_annotations_dir + "all-genes.faa",
        nt = genes_and_annotations_dir + "all-genes.fa",
        gff = genes_and_annotations_dir + "all-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule run_KO_annotation:
    """ runs KO functional annotation on each ref genome """

    input:
        genes_and_annotations_dir + "all-genes.faa"
    output:
        annotations = genes_and_annotations_dir + "KO-annotations.tsv"
    params:
        tmp_out = genes_and_annotations_dir + "KO-tab.tmp",
        tmp_dir = genes_and_annotations_dir + "tmp-KO-dir"
    shell:
        """
        exec_annotation -p ${{KO_DIR}}/profiles/ -k ${{KO_DIR}}/ko_list --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 
        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}
        rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule parse_coverage_and_detection:
    """
    This rule pulls out gene-level coverage and detection information for each sample,
    and filters the coverage information based on requiring at least 50% detection.

    It also pulls out genome-level coverage and detection, producing those tables, and 
    making one that is filtered such that if a genome's detectionis less than 50% in a sample,
    it's coverage is set to 0.
    """

    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa",
        ref = ref_genomes_dir + combined_fasta,
        genomes = ref_genome_IDs
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp",
        contig_level_tmp = mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv",
        genome_cov = mapping_dir + "{ID}-genome-level-coverages.tsv",
        genome_det = mapping_dir + "{ID}-genome-level-detections.tsv",
        filtered_genome_cov = mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} ref={input.ref} out={params.contig_level_tmp} > /dev/null 2>&1

        # filtering gene coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

        # parsing genome-level info
        python scripts/get-genome-level-coverage.py -i {params.contig_level_tmp} -g {input.genomes} -o {mapping_dir}{wildcards.ID}

          # removing intermediate files
        rm {params}
        """


rule parse_454_gene_level_cov_and_det:
    """
    This rule pulls out gene-level coverage and detection information for each 454 sample,
    and filters the coverage information based on requiring at least 50% detection.

    It also pulls out genome-level coverage and detection, producing those tables, and 
    making one that is filtered such that if a genome's detectionis less than 50% in a sample,
    it's coverage is set to 0.
    """

    input:
        bam = pyro_mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa",
        ref = ref_genomes_dir + combined_fasta,
        genomes = ref_genome_IDs
    params:
        gene_cov_and_det_tmp = pyro_mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = pyro_mapping_dir + "{ID}-gene-cov.tmp",
        contig_level_tmp = pyro_mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = pyro_mapping_dir + "{ID}-gene-coverages.tsv",
        genome_cov = pyro_mapping_dir + "{ID}-genome-level-coverages.tsv",
        genome_det = pyro_mapping_dir + "{ID}-genome-level-detections.tsv",
        filtered_genome_cov = pyro_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} ref={input.ref} out={params.contig_level_tmp} > /dev/null 2>&1

        # filtering gene coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

        # parsing genome-level info
        python scripts/get-genome-level-coverage.py -i {params.contig_level_tmp} -g {input.genomes} -o {pyro_mapping_dir}{wildcards.ID}

          # removing intermediate files
        rm {params}
        """


rule parse_gene_level_cov_and_det_HK_samples:
    """
    This rule pulls out gene-level coverage and detection information for each HK sample,
    and filters the coverage information based on requiring at least 50% detection.

    It also pulls out genome-level coverage and detection, producing those tables, and 
    making one that is filtered such that if a genome's detectionis less than 50% in a sample,
    it's coverage is set to 0.
    """

    input:
        bam = HK_mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa",
        ref = ref_genomes_dir + combined_fasta,
        genomes = ref_genome_IDs
    params:
        gene_cov_and_det_tmp = HK_mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = HK_mapping_dir + "{ID}-gene-cov.tmp",
        contig_level_tmp = HK_mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = HK_mapping_dir + "{ID}-gene-coverages.tsv",
        genome_cov = HK_mapping_dir + "{ID}-genome-level-coverages.tsv",
        genome_det = HK_mapping_dir + "{ID}-genome-level-detections.tsv",
        filtered_genome_cov = HK_mapping_dir + "{ID}-genome-level-detection-filtered-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} ref={input.ref} out={params.contig_level_tmp} > /dev/null 2>&1

        # filtering gene coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

        # parsing genome-level info
        python scripts/get-genome-level-coverage.py -i {params.contig_level_tmp} -g {input.genomes} -o {HK_mapping_dir}{wildcards.ID}

          # removing intermediate files
        rm {params}
        """


rule combine_ref_genomes:
    """ concatenates ref genomes """

    input:
        expand(ref_genomes_dir + "{ID}.fa", ID = ref_genome_ID_list)
    output:
        ref_genomes_dir + combined_fasta
    shell:
        """
        cat {input} > {output}
        """


rule build_bowtie2_index:
    """ Builds bowtie2 database """

    input:
        ref_genomes_dir + combined_fasta
    params:
        basename = mapping_dir + bowtie2_basename
    output:
        expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    log:
        mapping_dir + "bowtie2-build.log"
    shell:
        """
        bowtie2-build {input} {params.basename} > {log} 2>&1
        cp {output} {HK_mapping_dir}
        cp {output} {pyro_mapping_dir}
        """


rule dl_illumina_sample_and_run_mapping:
    """ Downloads and maps Illumina samples (stored with ERR numbers in our data files) one at a time """

    input:
        trigger = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        mapping_directory = mapping_dir.rstrip("/"),
        bowtie2_basename = mapping_dir + bowtie2_basename
    output:
        mapping_dir + "{ID}.bam"
    shell:
        """
        curr_run_accs=$(grep -w -m 1 "^{wildcards.ID}" {sample_to_mapping_file} | cut -f 2)
        bash scripts/dl-and-map-sample.sh {params.mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} ${{curr_run_accs}}
        """


rule dl_454_sample_and_run_mapping:
    """ Downloads and maps 454 samples (stored with ERR numbers in our data files) one at a time """

    input:
        trigger = expand(pyro_mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        pyro_mapping_directory = pyro_mapping_dir.rstrip("/"),
        bowtie2_basename = pyro_mapping_dir + bowtie2_basename
    output:
        pyro_mapping_dir + "{ID}.bam"
    shell:
        """
        curr_run_accs=$(grep -w -m 1 "^{wildcards.ID}" {sample_to_mapping_file} | cut -f 2)
        bash scripts/dl-and-map-454-sample.sh {params.pyro_mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} ${{curr_run_accs}}
        """


rule dl_HK_sample_and_run_mapping:
    """ Downloads and maps illumina samples (stored with SRR numbers in our data files) one at a time (those from the HK microbiome dataset) """

    input:
        trigger = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        mapping_directory = HK_mapping_dir.rstrip("/"),
        bowtie2_basename = HK_mapping_dir + bowtie2_basename
    output:
        HK_mapping_dir + "{ID}.bam"
    shell:
        """
        # getting links to forward and reverse reads
        forward_SRR_acc=$(grep "^{wildcards.ID}" {HK_sample_to_mapping_file} | cut -f 2)
        reverse_SRR_acc=$(grep "^{wildcards.ID}" {HK_sample_to_mapping_file} | cut -f 3)

        R1_link=$(grep "^${{forward_SRR_acc}}" {HK_SRR_to_link_mapping_file} | cut -f 2)
        R2_link=$(grep "^${{reverse_SRR_acc}}" {HK_SRR_to_link_mapping_file} | cut -f 2)

        bash scripts/dl-and-map-HK-sample.sh {params.mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} "${{R1_link}}" "${{R2_link}}"
        """


rule clean_all:
    shell:
        "rm -rf {dirs_to_create} .snakemake/ snakemake-run.log {ref_genomes_dir}{combined_fasta}"
