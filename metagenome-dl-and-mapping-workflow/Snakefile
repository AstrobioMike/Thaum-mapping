############################################################################################
## Snakefile for mapping metagenomic samples                                              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os


########################################
############# General Info #############
########################################

"""
There are only a few programs, so not using separate conda environments for individual rules.
All meant to be run in a conda environment that can be created as:
# conda create -n Thaum-mapping -c conda-forge -c bioconda -c defaults -c astrobiomike \
#              bowtie2=2.4.2 samtools=1.11 sra-tools=2.10.8 kofamscan=1.3.0 prodigal=2.6.3 \
#              bit=1.8.15 bbmap=38.86 snakemake=5.26.1
# conda activate Thaum-mapping

As written below, the KOFamScan db needs to be stored a shell environmental variable called: $KO_DIR
This can be added to a conda environment like so (when in the active conda environment):
mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d/
echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh

Where the HMMs would be: /path/to/kofamscan_db/profiles/
and the ko_list would be: /path/to/kofamscan_db/ko_list
"""


########################################
######## Setting some variables ########
########################################

  # single-column file holding reference genome IDs (extension expected to be .fa, but not in this file)
ref_genome_IDs = "all-ref-genome-IDs.txt"


  # single-column file holding unique portion of illumina sample names that process one way
sample_IDs_file = "unique-illumina-sample-IDs.txt"
pyro_sample_IDs_file = "unique-454-sample-IDs.txt"

  # single-column file of the Hong Kong MGs because they need to be processed differently (PRJNA588686)
HK_sample_IDs_file = "unique-HK-sample-IDs.txt"

  # tab-delimited mapping file holding our unique sample IDs in first column, 
  # and their corresponding ERR run accessions in the second column (for those being processed first way)
    # if multiple run accessions, they need to be comma-delimited, e.g.: ANE_004_05M	ERR598955,ERR599003
sample_to_mapping_file = "sample-and-run-accessions.tsv"

  # tab-delimited mapping file holding unique HK dataset IDs in first column,
  # and their corresponding SRR run accession for their forward read in the 2nd column,
  # and reverse read in 3rd column, e.g.: AT04-0m-8Ar	SRR10912837	SRR10912836
    # all of these had single accessions for forward and single for reverse
HK_sample_to_mapping_file = "HK-sample-and-run-accessions.tsv"


  # tab-delimited mapping file holding SRR run accession in first column,
  # and corresponding ftp download link in second column (for those that need to be procesed differently),
  #  e.g.: SRR10912786	ftp.sra.ebi.ac.uk/vol1/fastq/SRR109/086/SRR10912786/SRR10912786.fastq.gz
HK_SRR_to_link_mapping_file = "HK-SRRs-and-links.tsv"


  # fasta file of all combined ref genome fasta files
combined_fasta = "all-combined.fa"


  # bowtie2 index base name
bowtie2_basename = "all-combined"
  # bowtie2 index extensions
bowtie2_index_exts = [".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2"]


  # directories
ref_genomes_dir = "../ref-genomes/"
mapping_dir = "../mapping-files/"
HK_mapping_dir = "../mapping-files-HK/"
pyro_mapping_dir = "../mapping-files-pyro/"
genes_and_annotations_dir = "../genes-and-annotations/"
combined_gene_coverages = "../combined-gene-coverages/"

dirs_to_create = [mapping_dir, HK_mapping_dir, pyro_mapping_dir, genes_and_annotations_dir, combined_gene_coverages]


  # number of threads or cpus use PER snakemake job started (which is determined by the -j parameter passed to the snakemake call)
    # passed to bowtie2
num_threads = 4
    # passed to kofamscan
num_cpus = 4

########################################
#### Reading samples file into list ####
########################################

ref_genome_ID_list = [line.strip() for line in open(ref_genome_IDs)]
sample_ID_list = [line.strip() for line in open(sample_IDs_file)]
HK_sample_ID_list = [line.strip() for line in open(HK_sample_IDs_file)]
pyro_sample_ID_list = [line.strip() for line in open(pyro_sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in dirs_to_create:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        bowtie2_db = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts),
        reg_bam_files = expand(mapping_dir + "{ID}.bam", ID = sample_ID_list),
        pyro_bam_files = expand(pyro_mapping_dir + "{ID}.bam", ID = pyro_sample_ID_list),
        HK_bam_files = expand(HK_mapping_dir + "{ID}.bam", ID = HK_sample_ID_list),
        gene_covs = expand(mapping_dir + "{ID}-gene-coverages.tsv", ID = sample_ID_list),
        pyro_gene_covs = expand(pyro_mapping_dir + "{ID}-gene-coverages.tsv", ID = pyro_sample_ID_list),
        HK_gene_covs = expand(HK_mapping_dir + "{ID}-gene-coverages.tsv", ID = HK_sample_ID_list),
        annotations = genes_and_annotations_dir + "KO-annotations.tsv",
        combined_gene_covs = combined_gene_coverages + "Combined-gene-coverages.tsv",
        normalized_combined_gene_covs = combined_gene_coverages + "Combined-CPM-normalized-gene-coverages.tsv"


rule combine_gene_covs:
    """ generates tables of combined gene coverages """

    input:
        expand(mapping_dir + "{ID}-gene-coverages.tsv", ID = sample_ID_list),
        expand(pyro_mapping_dir + "{ID}-gene-coverages.tsv", ID = pyro_sample_ID_list),
        expand(HK_mapping_dir + "{ID}-gene-coverages.tsv", ID = HK_sample_ID_list)
    output:
        combined_gene_covs = combined_gene_coverages + "Combined-gene-coverages.tsv",
        normalized_combined_gene_covs = combined_gene_coverages + "Combined-CPM-normalized-gene-coverages.tsv"
    shell:
        """
        python scripts/combine-gene-level-coverages.py {input} -o {combined_gene_coverages}Combined
        """


rule call_genes:
    """ calls genes on ref genomes with prodigal """

    input:
        ref_genomes_dir + combined_fasta
    output:
        AA = genes_and_annotations_dir + "all-genes.faa",
        nt = genes_and_annotations_dir + "all-genes.fa",
        gff = genes_and_annotations_dir + "all-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule run_KO_annotation:
    """ runs KO functional annotation on each ref genome """

    input:
        genes_and_annotations_dir + "all-genes.faa"
    output:
        annotations = genes_and_annotations_dir + "KO-annotations.tsv"
    params:
        tmp_out = genes_and_annotations_dir + "KO-tab.tmp",
        tmp_dir = genes_and_annotations_dir + "tmp-KO-dir"
    shell:
        """
        exec_annotation -p ${{KO_DIR}}/profiles/ -k ${{KO_DIR}}/ko_list --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 
        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}
        rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule parse_gene_level_cov_and_det:
    """
    This rule pulls out gene-level coverage and detection information for each sample,
    and filters the coverage information based on requiring at least 50% detection.
    """

    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa"
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # removing intermediate files
        rm {params}
        """


rule parse_454_gene_level_cov_and_det:
    """
    This rule pulls out gene-level coverage and detection information for each 454 sample,
    and filters the coverage information based on requiring at least 50% detection.
    """

    input:
        bam = pyro_mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa"
    params:
        gene_cov_and_det_tmp = pyro_mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = pyro_mapping_dir + "{ID}-gene-cov.tmp"
    output:
        gene_covs = pyro_mapping_dir + "{ID}-gene-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # removing intermediate files
        rm {params}
        """


rule parse_gene_level_cov_and_det_HK_samples:
    """
    This rule pulls out gene-level coverage and detection information for each sample,
    and filters the coverage information based on requiring at least 50% detection.
    """

    input:
        bam = HK_mapping_dir + "{ID}.bam",
        nt = genes_and_annotations_dir + "all-genes.fa"
    params:
        gene_cov_and_det_tmp = HK_mapping_dir + "{ID}-gene-cov-and-det.tmp",
        gene_cov_tmp = HK_mapping_dir + "{ID}-gene-cov.tmp"
    output:
        gene_covs = HK_mapping_dir + "{ID}-gene-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # removing intermediate files
        rm {params}
        """


rule combine_ref_genomes:
    """ concatenates ref genomes """

    input:
        expand(ref_genomes_dir + "{ID}.fa", ID = ref_genome_ID_list)
    output:
        ref_genomes_dir + combined_fasta
    shell:
        """
        cat {input} > {output}
        """


rule build_bowtie2_index:
    """ Builds bowtie2 database """

    input:
        ref_genomes_dir + combined_fasta
    params:
        basename = mapping_dir + bowtie2_basename
    output:
        expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    log:
        mapping_dir + "bowtie2-build.log"
    shell:
        """
        bowtie2-build {input} {params.basename} > {log} 2>&1
        cp {output} {HK_mapping_dir}
        cp {output} {pyro_mapping_dir}
        """


rule dl_illumina_sample_and_run_mapping:
    """ Downloads and maps Illumina samples (stored with ERR numbers in our data files) one at a time """

    input:
        trigger = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        mapping_directory = mapping_dir.rstrip("/"),
        bowtie2_basename = mapping_dir + bowtie2_basename
    output:
        mapping_dir + "{ID}.bam"
    shell:
        """
        curr_run_accs=$(grep -w -m 1 "^{wildcards.ID}" {sample_to_mapping_file} | cut -f 2)
        bash scripts/dl-and-map-sample.sh {params.mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} ${{curr_run_accs}}
        """


rule dl_454_sample_and_run_mapping:
    """ Downloads and maps 454 samples (stored with ERR numbers in our data files) one at a time """

    input:
        trigger = expand(pyro_mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        pyro_mapping_directory = pyro_mapping_dir.rstrip("/"),
        bowtie2_basename = pyro_mapping_dir + bowtie2_basename
    output:
        pyro_mapping_dir + "{ID}.bam"
    shell:
        """
        curr_run_accs=$(grep -w -m 1 "^{wildcards.ID}" {sample_to_mapping_file} | cut -f 2)
        bash scripts/dl-and-map-454-sample.sh {params.pyro_mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} ${{curr_run_accs}}
        """


rule dl_HK_sample_and_run_mapping:
    """ Downloads and maps illumina samples (stored with SRR numbers in our data files) one at a time (those from the HK microbiome dataset) """

    input:
        trigger = expand(mapping_dir + bowtie2_basename + "{ext}", ext = bowtie2_index_exts)
    params:
        mapping_directory = HK_mapping_dir.rstrip("/"),
        bowtie2_basename = HK_mapping_dir + bowtie2_basename
    output:
        HK_mapping_dir + "{ID}.bam"
    shell:
        """
        # getting links to forward and reverse reads
        forward_SRR_acc=$(grep "^{wildcards.ID}" {HK_sample_to_mapping_file} | cut -f 2)
        reverse_SRR_acc=$(grep "^{wildcards.ID}" {HK_sample_to_mapping_file} | cut -f 3)

        R1_link=$(grep "^${{forward_SRR_acc}}" {HK_SRR_to_link_mapping_file} | cut -f 2)
        R2_link=$(grep "^${{reverse_SRR_acc}}" {HK_SRR_to_link_mapping_file} | cut -f 2)

        bash scripts/dl-and-map-HK-sample.sh {params.mapping_directory} {params.bowtie2_basename} {num_threads} {wildcards.ID} "${{R1_link}}" "${{R2_link}}"
        """


rule clean_all:
    shell:
        "rm -rf {dirs_to_create} .snakemake/ snakemake-run.log {ref_genomes_dir}{combined_fasta}"
